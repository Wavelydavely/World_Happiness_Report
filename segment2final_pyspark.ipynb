{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segment2final_pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj2UZxvE-EyZ",
        "outputId": "a4186864-21a4-409a-90b9-15cf06a7ac13"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.1.2'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,263 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [510 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,421 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:19 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [66.2 kB]\n",
            "Ign:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [695 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,786 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [39.4 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,699 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [914 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [544 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,196 kB]\n",
            "Get:28 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:29 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.1 kB]\n",
            "Fetched 13.6 MB in 5s (2,961 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwtxzqLPfnq-",
        "outputId": "dec86a84-c4a2-4e2f-9458-f83cbc4ace59"
      },
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#  Import and read the csv.\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"World-Happiness\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()\n",
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "url =\"https://worldhappiness.s3.us-east-2.amazonaws.com/WHR_noNull.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "happiness_df = spark.read.csv(SparkFiles.get(\"WHR_noNull.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "happiness_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----+--------------------+---------------+-------------------+--------------+---------------+-------+------------------+-------------------------+\n",
            "|    country|year|        world_region|happiness_score|economic_production|social_support|life_expectancy|freedom|        generosity|perceptions_of_corruption|\n",
            "+-----------+----+--------------------+---------------+-------------------+--------------+---------------+-------+------------------+-------------------------+\n",
            "|    Denmark|2005|      Western Europe|          8.019|             10.851|         0.972|           69.6|  0.971|0.1615333333333333|                    0.237|\n",
            "|    Denmark|2008|      Western Europe|          7.971|              10.88|         0.954|          70.08|   0.97|             0.272|                    0.248|\n",
            "|    Finland|2020|      Western Europe|          7.889|              10.75|         0.962|           72.1|  0.962|            -0.116|                    0.164|\n",
            "|    Finland|2018|      Western Europe|          7.858|             10.783|         0.962|           71.9|  0.938|            -0.127|                    0.199|\n",
            "|    Finland|2021|      Western Europe|          7.842|             10.775|         0.954|           72.0|  0.949|            -0.098|                    0.186|\n",
            "|    Denmark|2007|      Western Europe|          7.834|             10.891|         0.954|          69.92|  0.932|              0.24|                    0.206|\n",
            "|    Finland|2017|      Western Europe|          7.788|             10.768|         0.964|           71.8|  0.962|            -0.002|                    0.192|\n",
            "|    Denmark|2011|      Western Europe|          7.788|             10.848|         0.962|          70.62|  0.935|             0.298|                     0.22|\n",
            "|    Finland|2019|      Western Europe|           7.78|             10.792|         0.937|           72.0|  0.948|            -0.052|                    0.195|\n",
            "|Switzerland|2012|      Western Europe|          7.776|             11.079|         0.947|          72.78|  0.945|             0.139|                    0.323|\n",
            "|    Denmark|2010|      Western Europe|          7.771|             10.839|         0.975|           70.4|  0.944|             0.242|                    0.175|\n",
            "|Switzerland|2019|      Western Europe|          7.694|             11.136|         0.949|           74.4|  0.913|             0.036|                    0.294|\n",
            "|    Denmark|2019|      Western Europe|          7.693|             10.954|         0.958|           72.7|  0.963|              0.02|                    0.174|\n",
            "|    Denmark|2009|      Western Europe|          7.683|             10.824|         0.939|          70.24|  0.949|             0.264|                    0.206|\n",
            "|     Norway|2012|      Western Europe|          7.678|             11.017|         0.948|          72.24|  0.947|             0.147|                    0.368|\n",
            "|    Finland|2006|      Western Europe|          7.672|             10.745|         0.965|          69.76|  0.969|            -0.005|                    0.132|\n",
            "|    Finland|2008|      Western Europe|          7.671|             10.796|         0.951|          70.08|  0.934|             0.028|                    0.217|\n",
            "|    Finland|2016|      Western Europe|           7.66|              10.74|         0.954|           71.7|  0.948|            -0.027|                     0.25|\n",
            "|     Canada|2010|North America and...|           7.65|             10.717|         0.954|           72.2|  0.934|              0.23|                    0.413|\n",
            "|    Denmark|2018|      Western Europe|          7.649|             10.935|         0.958|           72.4|  0.935|             0.018|                    0.151|\n",
            "+-----------+----+--------------------+---------------+-------------------+--------------+---------------+-------+------------------+-------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEEIi0F4f5ao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "a0c0c61e-ff97-4f63-a258-55859b44d5df"
      },
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "y = happiness_df[\"happiness_score\"] # y = dependent = ladder score\n",
        "X = happiness_df[[\"economic_production\", \"social_support\", \"life_expectancy\", \"freedom\", \"generosity\", \"perceptions_of_corruption\"]] # X = independent = any other variable\n",
        "\n",
        "\n",
        "\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-adb2bef5e95b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Split the preprocessed data into a training and testing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m78\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2116\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2118\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'pyspark.sql.dataframe.DataFrame'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H506Qv-Df5dP"
      },
      "source": [
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74e58P-Tf5fk"
      },
      "source": [
        "# Introduce regressor\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRUpGl3lf5h3"
      },
      "source": [
        "# Print the intercept and coefficients\n",
        "print(\"Intercept: \", regressor.intercept_)\n",
        "print(\"Coefficients:\")\n",
        "list(zip(X, regressor.coef_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhqZAHN5f5kJ"
      },
      "source": [
        "# Create vector of predictions\n",
        "y_pred = regressor.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBU2Lt7Ff5mZ"
      },
      "source": [
        "print(f\"Prediction: {y_pred}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdraizsPf5op"
      },
      "source": [
        "# Show actual value with predicted value\n",
        "regressor_diff = pd.DataFrame({'Actual value': y_test, 'Predicted value': y_pred})\n",
        "values = regressor_diff.sort_values('Actual value', ascending=False)\n",
        "values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NFh7ACVf5re"
      },
      "source": [
        "#Model Evaluation\n",
        "from sklearn import metrics\n",
        "MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
        "MSE = metrics.mean_squared_error(y_test, y_pred)\n",
        "RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('R squared: {:.2f}'.format(regressor.score(X, y)))\n",
        "print('Mean Absolute Error:', MAE)\n",
        "print('Mean Square Error:', MSE)\n",
        "print('Root Mean Square Error:', RMSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d_tgYaif5tJ"
      },
      "source": [
        "actual = values['Actual value']\n",
        "predicted = values['Predicted value']\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(24, 12))\n",
        "\n",
        "ax.plot(actual, color = 'green', label = 'Actual Score')\n",
        "ax.plot(predicted, color = 'red', label = 'Predicted Score')\n",
        "ax.set(xlabel='', ylabel='Happiness Score',\n",
        "       title='Actual vs Predicted Happiness Scores');\n",
        "ax.legend(loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2-IIR3tf5vR"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "fig, axs = plt.subplots(6, figsize = (8,8))\n",
        "plt1 = sns.boxplot(happiness_df['economic_production'], ax = axs[0])\n",
        "plt2 = sns.boxplot(happiness_df['social_support'], ax = axs[1])\n",
        "plt3 = sns.boxplot(happiness_df['life_expectancy'], ax = axs[2])\n",
        "plt4 = sns.boxplot(happiness_df['freedom'], ax = axs[3])\n",
        "plt5 = sns.boxplot(happiness_df['generosity'], ax = axs[4])\n",
        "plt6 = sns.boxplot(happiness_df['perceptions_of_corruption'], ax = axs[5])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdkao9DDf5xh"
      },
      "source": [
        "sns.pairplot(happiness_df, x_vars=[\"economic_production\", \"social_support\", \"life_expectancy\"],\n",
        "             y_vars=\"happiness_score\", height=7, aspect=1, kind='scatter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZJA0mHxf5z5"
      },
      "source": [
        "sns.pairplot(happiness_df, x_vars=[\"freedom\", \"generosity\", \"perceptions_of_corruption\"],\n",
        "             y_vars=\"happiness_score\", height=7, aspect=1, kind='scatter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHw1coLRiBf2"
      },
      "source": [
        "sns.pairplot(happiness_df, x_vars=\"world_region\",\n",
        "             y_vars=\"happiness_score\", height=10, aspect=2, kind='scatter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6RssvDqgJwd"
      },
      "source": [
        "# RANDOM FOREST MODEL\n",
        "\n",
        "import pandas as pd\n",
        "#from path import Path\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Read data and store into a Pandas DataFrame\n",
        "url = \"https://raw.githubusercontent.com/Wavelydavely/World_Happiness_Report/main/Cleaned_Data/WHR_noNull.csv\"\n",
        "happiness_no_null = pd.read_csv(url)\n",
        "happiness_no_null.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX43B4BtgRNc"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df2 = happiness_no_null.copy()\n",
        "df2['country'] = le.fit_transform(df2['country'])\n",
        "df2.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4xEGSfAgWKd"
      },
      "source": [
        "#from sklearn.preprocessing import LabelEncoder\n",
        "#le = LabelEncoder()\n",
        "df3 = df2.copy()\n",
        "df3['year'] = le.fit_transform(df2['year'])\n",
        "df3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT8MAFHGgnOI"
      },
      "source": [
        "#from sklearn.preprocessing import LabelEncoder\n",
        "#le = LabelEncoder()\n",
        "df4 = df3.copy()\n",
        "df4['world_region'] = le.fit_transform(df3['world_region'])\n",
        "df4.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBQByGBfgp5S"
      },
      "source": [
        "# Define the features set.\n",
        "X = df4.copy()\n",
        "X = X.drop(\"happiness_score\", axis=1)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y04t6Xhdgrzq"
      },
      "source": [
        "# Define the target set.\n",
        "y = df4[\"happiness_score\"].ravel()\n",
        "y[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WauJf56Lgt-Q"
      },
      "source": [
        "# Splitting into Train and Test sets.\n",
        "X_train, X_test, y_train, y_test2 = train_test_split(X, y, random_state=78)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqCtBCSQgwFc"
      },
      "source": [
        "# Creating a StandardScaler instance.\n",
        "scaler = StandardScaler()\n",
        "# Fitting the Standard Scaler with the training data.\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scaling the data.\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbKo8YZTgx57"
      },
      "source": [
        "# Create a random forest classifier.\n",
        "rf_model = RandomForestRegressor(n_estimators=128, random_state=78) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMas4LpEgz7p"
      },
      "source": [
        "# Fitting the model\n",
        "rf_model = rf_model.fit(X_train_scaled, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZwVeMMeg2F2"
      },
      "source": [
        "# Making predictions using the testing data.\n",
        "predictions = rf_model.predict(X_test_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckws86yzg4iI"
      },
      "source": [
        "#Model Evaluation\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import metrics\n",
        "MAE = metrics.mean_absolute_error(y_test2, predictions)\n",
        "MSE = metrics.mean_squared_error(y_test2, predictions)\n",
        "RMSE = np.sqrt(metrics.mean_squared_error(y_test2, predictions))\n",
        "print('R squared: {:.2f}'.format(rf_model.score(X, y)))\n",
        "print('Mean Absolute Error:', MAE)\n",
        "print('Mean Square Error:', MSE)\n",
        "print('Root Mean Square Error:', RMSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zEZVOwEg-J4"
      },
      "source": [
        "# Calculate feature importance in the Random Forest model.\n",
        "importances = rf_model.feature_importances_\n",
        "importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXasq81xhCN9"
      },
      "source": [
        "# We can sort the features by their importance.\n",
        "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxieeIBdvzOj"
      },
      "source": [
        "# Create vector of predictions\n",
        "y_pred2 = rf_model.predict(X_test)\n",
        "y_pred2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pVUfQeByrt2"
      },
      "source": [
        "# Show actual value with predicted value\n",
        "regressor_diff2 = pd.DataFrame({'Actual value': y_test2, 'Predicted value': y_pred2})\n",
        "values2 = regressor_diff2.sort_values('Predicted value', ascending=False)\n",
        "values2.reset_index(drop=True, inplace=True)\n",
        "#regressor_diff2\n",
        "values2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKaYMxynuNZt"
      },
      "source": [
        "actual = values2['Actual value']\n",
        "predicted = values2['Predicted value']\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(24, 12))\n",
        "\n",
        "ax.plot(actual, color = 'green', label = 'Actual Score')\n",
        "ax.plot(predicted, color = 'red', label = 'Predicted Score')\n",
        "ax.set(xlabel='', ylabel='Happiness Score',\n",
        "       title='Actual vs Predicted Happiness Scores');\n",
        "ax.legend(loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqIXmPzD-4XG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}